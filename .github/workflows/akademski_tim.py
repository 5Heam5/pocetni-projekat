import os
import json
import time
import re
from openai import AsyncOpenAI
from pydantic import BaseModel, Field
from typing import Union, List, Dict, Any


# Koristimo pydantic.Field za bolju integraciju sa OpenWebUI
class Valves(BaseModel):
    # Osnovna AI podesavanja
    OLLAMA_URL: str = Field(
        default="http://host.docker.internal:11434/v1",
        description="URL Ollama API-ja",
        json_schema_extra={"group": "Osnovna podesavanja", "order": 1},
    )
    MODEL_AKADEMIK: str = Field(
        default="qwen2-vl:7b",
        description="Model za analizu i izradu outline-a (Akademik)",
        json_schema_extra={"group": "Modeli", "order": 2},
    )
    MODEL_PISAC: str = Field(
        default="mistral-nemo:12b",
        description="Model za pisanje prvog drafta (Pisac)",
        json_schema_extra={"group": "Modeli", "order": 3},
    )
    MODEL_LEKTOR: str = Field(
        default="gemma3:27b-it-qat",
        description="Model za konacno lekturanje i formatiranje (Lektor)",
        json_schema_extra={"group": "Modeli", "order": 4},
    )
    TEMPERATURE: float = Field(
        default=0.3,
        ge=0.0,
        le=1.0,
        description="Kreativnost modela (0.0 - 1.0)",
        json_schema_extra={"group": "Postavke modela", "order": 5},
    )
    MAX_TOKENS: int = Field(
        default=16000,
        description="Maksimalan broj tokena za odgovor",
        json_schema_extra={"group": "Postavke modela", "order": 6},
    )
    MEMORY_TTL_SECONDS: int = Field(
        default=3600,
        description="Vreme zivota memorije u sekundama",
        json_schema_extra={"group": "Napredna podesavanja", "order": 7},
    )
    MAX_TOKENS_PER_CHUNK: int = Field(
        default=8000,
        description="Maksimalan broj tokena po delu za lekturanje",
        json_schema_extra={"group": "Napredna podesavanja", "order": 8},
    )


class Pipe:
    """
    Akademski Tim 2025 â€“ Triple Threat Edition.
    Procesuiranje teksta kroz tri faze: Akademik (analiza), Pisac (draft), Lektor (final).
    """

    def __init__(self):
        self.valves = Valves()
        self.client = AsyncOpenAI(base_url=self.valves.OLLAMA_URL, api_key="ollama")
        self.type = "manifold"
        self.memory = {}

    def pipes(self):
        return [{"id": "triple_v2", "name": "Akademski Tim 2025 â€“ Triple Threat v2"}]

    async def get_valves(self):
        return self.valves.dict()

    async def update_valves(self, **valves):
        try:
            for key, value in valves.items():
                if hasattr(self.valves, key):
                    setattr(self.valves, key, value)

            self.client = AsyncOpenAI(base_url=self.valves.OLLAMA_URL, api_key="ollama")
            return await self.get_valves()
        except Exception as e:
            raise Exception(f"Greska pri azuriranju podesavanja: {str(e)}")

    def _cleanup_memory(self):
        """Ciscenje stare memorije."""
        current_time = time.time()
        to_delete = [
            chat_id
            for chat_id, data in self.memory.items()
            if current_time - data.get("timestamp", 0) > self.valves.MEMORY_TTL_SECONDS
        ]
        for chat_id in to_delete:
            del self.memory[chat_id]

    def _estimate_tokens(self, text: str) -> int:
        """Gruba procena broja tokena (proseÄno 4 karaktera po tokenu)."""
        return len(text) // 4

    def _split_text_into_chunks(self, text: str, max_tokens: int) -> List[str]:
        """Deljenje teksta na manje delove (chunk-ove) na osnovu broja tokena."""
        paragraphs = re.split(r"\n\s*\n", text.strip())
        chunks = []
        current_chunk = []
        current_tokens = 0

        for para in paragraphs:
            para_tokens = self._estimate_tokens(para)
            if current_tokens + para_tokens > max_tokens and current_chunk:
                chunks.append("\n\n".join(current_chunk))
                current_chunk = [para]
                current_tokens = para_tokens
            else:
                current_chunk.append(para)
                current_tokens += para_tokens

        if current_chunk:
            chunks.append("\n\n".join(current_chunk))

        return chunks

    async def _stream(self, model: str, prompt: Union[str, list[dict]], emitter):
        """Pomocna funkcija za streamovanje odgovora od modela."""
        messages = (
            [{"role": "user", "content": prompt}] if isinstance(prompt, str) else prompt
        )
        try:
            stream = await self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=self.valves.TEMPERATURE,
                max_tokens=self.valves.MAX_TOKENS,
                stream=True,
            )
            full_response = ""
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    await emitter({"type": "content", "content": content})
            return full_response
        except Exception as e:
            error_message = (
                f"\n\n[GRESKA] Komunikacija sa modelom '{model}' neuspesna: {str(e)}\n"
            )
            await emitter({"type": "content", "content": error_message})
            print(f"Greska u pipeline-u: {e}")
            return ""

    async def pipe(self, body: dict, __event_emitter__):
        """GLAVNA METODA - OpenWebUI poziva ovu metodu kada se pokrene pipeline"""
        self._cleanup_memory()

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ OSNOVNA VALIDACIJA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if not body.get("messages"):
            await __event_emitter__(
                {"type": "content", "content": "GRESKA: Nema poruka u zahtevu."}
            )
            return

        user_msg = body["messages"][-1]["content"] if body.get("messages") else ""
        if not user_msg or not str(user_msg).strip():
            await __event_emitter__(
                {
                    "type": "content",
                    "content": "GRESKA: Prazan tekst. Molimo unesite sadrÅ¾ajan upit.",
                }
            )
            return

        chat_id = body.get("chat_id", "default")
        images = []

        # Obrada slika iz poruka
        for msg in body.get("messages", []):
            if isinstance(msg.get("content"), list):
                for part in msg["content"]:
                    if part.get("type") == "image_url" and part.get(
                        "image_url", {}
                    ).get("url"):
                        images.append(part["image_url"]["url"])

        await __event_emitter__(
            {
                "type": "content",
                "content": "ğŸš€ Pokrecem Akademski Tim 2025 â€“ Triple Threat Edition v2...\n\n",
            }
        )

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. AKADEMIK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        await __event_emitter__(
            {
                "type": "content",
                "content": "1ï¸âƒ£ **AKADEMIK** â€“ Analiza teme i izrada outline-a\n",
            }
        )
        akademik_content = [
            {
                "type": "text",
                "text": f"Napravi detaljan akademski plan i outline (strukturu) za temu:\n\n{user_msg}",
            }
        ]
        # Dodaj slike ako postoje
        if images:
            for img in images:
                akademik_content.append(
                    {"type": "image_url", "image_url": {"url": img}}
                )

        akademik_msg = [{"role": "user", "content": akademik_content}]

        outline = await self._stream(
            self.valves.MODEL_AKADEMIK, akademik_msg, __event_emitter__
        )
        if not outline.strip():
            await __event_emitter__(
                {
                    "type": "content",
                    "content": "\n\nâŒ GRESKA: Akademik nije generisao outline. Proces se prekida.",
                }
            )
            return

        self.memory[chat_id] = {"outline": outline, "timestamp": time.time()}
        await __event_emitter__({"type": "content", "content": "\n\n---\n\n"})

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. PISAC â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        await __event_emitter__(
            {"type": "content", "content": "2ï¸âƒ£ **PISAC** â€“ Pisanje prvog drafta\n"}
        )
        pisac_prompt = f"""PiÅ¡i kompletan akademski rad na srpskom jeziku.
Striktno prati sledeci outline:
{outline}

Zahtevi:
- Originalan, naucni stil.
- Koristi APA 7 stil citiranja (ne izmiÅ¡ljaj reference, ako nemaÅ¡ informacije, koristi opÅ¡te formate).
- Ciljna duÅ¾ina: 2500â€“4000 reci.
- Fokusiraj se na sadrÅ¾aj, ne na formatiranje."""

        draft = await self._stream(
            self.valves.MODEL_PISAC, pisac_prompt, __event_emitter__
        )
        if not draft.strip():
            await __event_emitter__(
                {
                    "type": "content",
                    "content": "\n\nâŒ GRESKA: Pisac nije generisao draft. Proces se prekida.",
                }
            )
            return

        self.memory[chat_id]["draft"] = draft
        self.memory[chat_id]["timestamp"] = time.time()
        await __event_emitter__({"type": "content", "content": "\n\n---\n\n"})

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. LEKTOR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        await __event_emitter__(
            {
                "type": "content",
                "content": "3ï¸âƒ£ **LEKTOR** â€“ Finalna obrada, lekturanje i formatiranje\n",
            }
        )

        draft_tokens = self._estimate_tokens(draft)
        await __event_emitter__(
            {
                "type": "content",
                "content": f"[INFO] Draft ima ~{draft_tokens} tokena. Proveravam da li je potrebno deljenje...\n",
            }
        )

        if draft_tokens > self.valves.MAX_TOKENS_PER_CHUNK:
            await __event_emitter__(
                {
                    "type": "content",
                    "content": f"âœ‚ï¸ Tekst je predugacak, delimo ga na manje delove za lekturanje...\n",
                }
            )
            final_parts = []
            chunks = self._split_text_into_chunks(
                draft, self.valves.MAX_TOKENS_PER_CHUNK
            )

            for i, chunk in enumerate(chunks):
                is_last = i == len(chunks) - 1
                await __event_emitter__(
                    {
                        "type": "content",
                        "content": f"\nğŸ”¨ Obradjujem deo {i+1}/{len(chunks)}...\n",
                    }
                )

                if is_last:
                    lektor_prompt = f"""Ti si konaÄni lektor i urednik.
Zadatak za POSLEDNJI deo teksta:
1. Popravi gramatiku, stil, ponavljanja.
2. Osiguraj 100% srpski jezik.
3. DODAJ na sam pocetak celog rada (ne samo ovog dela) saÅ¾etak (Abstract) i kljuÄne reci.
4. Na sam kraj celog rada (ne samo ovog dela) dodaj formatiranu Literaturu prema APA 7 standardu.
5. Vrati ISKLJUCIVO obraÄ‘en deo teksta.

Deo za lekturanje:
{chunk}"""
                else:
                    lektor_prompt = f"""Ti si lektor i urednik.
Zadatak za SREDNJI deo teksta:
1. Popravi gramatiku, stil, ponavljanja.
2. Osiguraj 100% srpski jezik.
3. NE DODAVAJ saÅ¾etak, kljuÄne reci ili literaturu. To ce biti uraÄ‘eno na kraju.
4. Vrati ISKLJUCIVO obraÄ‘en deo teksta.

Deo za lekturanje:
{chunk}"""

                final_part = await self._stream(
                    self.valves.MODEL_LEKTOR, lektor_prompt, __event_emitter__
                )
                if final_part:
                    final_parts.append(final_part)

            final = "\n\n".join(final_parts)

        else:
            lektor_prompt = f"""Ti si konaÄni lektor i urednik.
Zadatak:
1. Popravi gramatiku, stil, ponavljanja u celom tekstu.
2. Formatiraj naslove i citate prema APA 7 standardu.
3. Osigurati 100% srpski jezik.
4. Dodati na pocetak saÅ¾etak (Abstract) i kljuÄne reci.
5. Dodati na kraj formatiranu Literaturu.
6. Vrati konaÄan rad spreman za predaju.

Draft:
{draft}"""
            final = await self._stream(
                self.valves.MODEL_LEKTOR, lektor_prompt, __event_emitter__
            )

        await __event_emitter__(
            {
                "type": "content",
                "content": f"\n\nğŸ‰ FINALNI MASTER RAD JE SPREMAN!\n\n{final}",
            }
        )

        # Ciscenje memorije za zavrÅ¡en chat
        if chat_id in self.memory:
            del self.memory[chat_id]
